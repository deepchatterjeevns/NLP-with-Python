{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbots.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/NLP-with-Python/blob/master/Chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdO_QceJwsOF",
        "colab_type": "text"
      },
      "source": [
        "##Creating Chatbots with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaceZY9Zxgas",
        "colab_type": "text"
      },
      "source": [
        "## End to End Representataion \n",
        "\n",
        "* Input Memory Representation\n",
        "\n",
        "* Output Memory Representation\n",
        "\n",
        "* Generating Final Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDD5M0IYy3BU",
        "colab_type": "text"
      },
      "source": [
        "## PART -1 \n",
        "\n",
        "* Load the Data\n",
        "* Explore the Data Format\n",
        "* Create a Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L5FIl7Bwpzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "with open('/content/train_qa.txt','rb') as f:\n",
        "  train_data = pickle.load(f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvLi8sy_2x-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/test_qa.txt','rb') as f:\n",
        "  test_data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzUs_oko2470",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e1162e6-3343-454e-8114-4bd97edd864b"
      },
      "source": [
        "type(test_data),type(train_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, list)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwxViGPs2-j2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf7195a-98e1-4abb-a0f3-15a6d80fd0ee"
      },
      "source": [
        "len(train_data),len(test_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYpUef4U3Dy8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8a0231e0-7631-4d35-d4f8-b20f5fb4d777"
      },
      "source": [
        "train_data[0]#story,#question,#answer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Mary',\n",
              "  'moved',\n",
              "  'to',\n",
              "  'the',\n",
              "  'bathroom',\n",
              "  '.',\n",
              "  'Sandra',\n",
              "  'journeyed',\n",
              "  'to',\n",
              "  'the',\n",
              "  'bedroom',\n",
              "  '.'],\n",
              " ['Is', 'Sandra', 'in', 'the', 'hallway', '?'],\n",
              " 'no')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4eeYz4m3TM7",
        "colab_type": "text"
      },
      "source": [
        "##Story"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed8zL6aH3RiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dca85d3f-3edd-4cd3-e5ab-2f0eff476722"
      },
      "source": [
        "' '.join(train_data[0][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mary moved to the bathroom . Sandra journeyed to the bedroom .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrLzgFww3ZU9",
        "colab_type": "text"
      },
      "source": [
        "##Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWdPCaqh3bGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab9287cb-16fd-418d-9c49-a840a7f3154e"
      },
      "source": [
        "' '.join(train_data[0][1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Is Sandra in the hallway ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEv4uuIP3eJr",
        "colab_type": "text"
      },
      "source": [
        "##Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvBZFz373f3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c117956-882e-4554-8639-fb43f9956b7e"
      },
      "source": [
        "''.join(train_data[0][2])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBBT833C3sB2",
        "colab_type": "text"
      },
      "source": [
        "##Setup a vocabulary of all the words in the document "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wolvcx9a3wQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = train_data + test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtLoqKFX4VPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54ba2a7d-12e7-4471-e8be-6765afd7ad2a"
      },
      "source": [
        "len(all_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZwk0reZ4uBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set()\n",
        "for story,question,answer in all_data:\n",
        "  vocab = vocab.union(set(story))\n",
        "  vocab = vocab.union(set(question))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3dH4dq_5CLq",
        "colab_type": "text"
      },
      "source": [
        "####add answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwwjJJ8H5BFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.add('no')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_PtkQqU5Pje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab.add('yes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn1LBmyy6KHC",
        "colab_type": "text"
      },
      "source": [
        "##set of all possible vocab words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m6c1CnB6C5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "84d3425c-a721-442a-e785-46da2055af99"
      },
      "source": [
        "vocab"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " '?',\n",
              " 'Daniel',\n",
              " 'Is',\n",
              " 'John',\n",
              " 'Mary',\n",
              " 'Sandra',\n",
              " 'apple',\n",
              " 'back',\n",
              " 'bathroom',\n",
              " 'bedroom',\n",
              " 'discarded',\n",
              " 'down',\n",
              " 'dropped',\n",
              " 'football',\n",
              " 'garden',\n",
              " 'got',\n",
              " 'grabbed',\n",
              " 'hallway',\n",
              " 'in',\n",
              " 'journeyed',\n",
              " 'kitchen',\n",
              " 'left',\n",
              " 'milk',\n",
              " 'moved',\n",
              " 'no',\n",
              " 'office',\n",
              " 'picked',\n",
              " 'put',\n",
              " 'the',\n",
              " 'there',\n",
              " 'to',\n",
              " 'took',\n",
              " 'travelled',\n",
              " 'up',\n",
              " 'went',\n",
              " 'yes'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbKfgROk6yOS",
        "colab_type": "text"
      },
      "source": [
        "#### add 1 to the length of vocab in order to be used for Keras padding sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq_qM8LC69Kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_len = len(vocab) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLtbT9WQ7IcL",
        "colab_type": "text"
      },
      "source": [
        "* Figure out how long is the longest question \n",
        "\n",
        "* Figure out how long is the longest story"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-muwACe7Uk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_story_len = [len(data[0]) for data in all_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9Ie_qXF78aO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68c581ac-5345-4898-de93-ff9fdfcf2d00"
      },
      "source": [
        "all_story_len"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12,\n",
              " 26,\n",
              " 39,\n",
              " 52,\n",
              " 64,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 63,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 24,\n",
              " 37,\n",
              " 48,\n",
              " 71,\n",
              " 83,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 35,\n",
              " 46,\n",
              " 71,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 13,\n",
              " 25,\n",
              " 39,\n",
              " 50,\n",
              " 62,\n",
              " 13,\n",
              " 26,\n",
              " 38,\n",
              " 49,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 26,\n",
              " 39,\n",
              " 51,\n",
              " 62,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 52,\n",
              " 65,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 48,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 51,\n",
              " 64,\n",
              " 12,\n",
              " 25,\n",
              " 39,\n",
              " 53,\n",
              " 65,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 61,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 60,\n",
              " 72,\n",
              " 13,\n",
              " 25,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 23,\n",
              " 36,\n",
              " 47,\n",
              " 59,\n",
              " 12,\n",
              " 24,\n",
              " 35,\n",
              " 48,\n",
              " 61,\n",
              " 12,\n",
              " 23,\n",
              " 36,\n",
              " 49,\n",
              " 62,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 48,\n",
              " 59,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 60,\n",
              " 12,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 35,\n",
              " 48,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 60,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 47,\n",
              " 60,\n",
              " 13,\n",
              " 25,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 38,\n",
              " 49,\n",
              " 60,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 51,\n",
              " 63,\n",
              " 24,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 73,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 51,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 23,\n",
              " 34,\n",
              " 47,\n",
              " 59,\n",
              " 13,\n",
              " 26,\n",
              " 38,\n",
              " 49,\n",
              " 60,\n",
              " 12,\n",
              " 23,\n",
              " 37,\n",
              " 49,\n",
              " 63,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 26,\n",
              " 38,\n",
              " 50,\n",
              " 61,\n",
              " 74,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 48,\n",
              " 60,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 59,\n",
              " 71,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 51,\n",
              " 64,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 59,\n",
              " 71,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 62,\n",
              " 107,\n",
              " 119,\n",
              " 131,\n",
              " 143,\n",
              " 156,\n",
              " 12,\n",
              " 26,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 23,\n",
              " 35,\n",
              " 47,\n",
              " 58,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 62,\n",
              " 13,\n",
              " 26,\n",
              " 38,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 24,\n",
              " 36,\n",
              " 50,\n",
              " 63,\n",
              " 13,\n",
              " 26,\n",
              " 39,\n",
              " 50,\n",
              " 63,\n",
              " 13,\n",
              " 26,\n",
              " 38,\n",
              " 52,\n",
              " 64,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 13,\n",
              " 26,\n",
              " 39,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 13,\n",
              " 25,\n",
              " 39,\n",
              " 50,\n",
              " 63,\n",
              " 12,\n",
              " 25,\n",
              " 38,\n",
              " 49,\n",
              " 61,\n",
              " 23,\n",
              " 35,\n",
              " 47,\n",
              " 58,\n",
              " 70,\n",
              " 13,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 59,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 62,\n",
              " 74,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 74,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 35,\n",
              " 49,\n",
              " 73,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 62,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 36,\n",
              " 48,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 39,\n",
              " 52,\n",
              " 64,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 60,\n",
              " 25,\n",
              " 36,\n",
              " 48,\n",
              " 59,\n",
              " 72,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 48,\n",
              " 61,\n",
              " 13,\n",
              " 26,\n",
              " 38,\n",
              " 49,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 13,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 63,\n",
              " 12,\n",
              " 23,\n",
              " 35,\n",
              " 47,\n",
              " 59,\n",
              " 23,\n",
              " 36,\n",
              " 47,\n",
              " 59,\n",
              " 72,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 62,\n",
              " 14,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 51,\n",
              " 64,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 14,\n",
              " 26,\n",
              " 38,\n",
              " 50,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 36,\n",
              " 47,\n",
              " 59,\n",
              " 12,\n",
              " 24,\n",
              " 38,\n",
              " 51,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 60,\n",
              " 13,\n",
              " 26,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 35,\n",
              " 46,\n",
              " 58,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 62,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 59,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 62,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 14,\n",
              " 27,\n",
              " 40,\n",
              " 53,\n",
              " 66,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 35,\n",
              " 46,\n",
              " 59,\n",
              " 71,\n",
              " 84,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 64,\n",
              " 12,\n",
              " 25,\n",
              " 36,\n",
              " 47,\n",
              " 59,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 23,\n",
              " 35,\n",
              " 47,\n",
              " 59,\n",
              " 71,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 26,\n",
              " 38,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 13,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 59,\n",
              " 13,\n",
              " 25,\n",
              " 36,\n",
              " 48,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 13,\n",
              " 26,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 60,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 60,\n",
              " 72,\n",
              " 14,\n",
              " 27,\n",
              " 39,\n",
              " 51,\n",
              " 63,\n",
              " 13,\n",
              " 26,\n",
              " 37,\n",
              " 48,\n",
              " 61,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 14,\n",
              " 26,\n",
              " 38,\n",
              " 50,\n",
              " 63,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 36,\n",
              " 49,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 35,\n",
              " 47,\n",
              " 60,\n",
              " 12,\n",
              " 23,\n",
              " 35,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 72,\n",
              " 13,\n",
              " 26,\n",
              " 39,\n",
              " 51,\n",
              " 63,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 74,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 47,\n",
              " 59,\n",
              " 14,\n",
              " 26,\n",
              " 39,\n",
              " 52,\n",
              " 64,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 26,\n",
              " 39,\n",
              " 51,\n",
              " 61,\n",
              " 24,\n",
              " 36,\n",
              " 47,\n",
              " 60,\n",
              " 72,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 72,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 48,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 35,\n",
              " 47,\n",
              " 60,\n",
              " 23,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 73,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 62,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 13,\n",
              " 25,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 60,\n",
              " 13,\n",
              " 26,\n",
              " 39,\n",
              " 51,\n",
              " 63,\n",
              " 13,\n",
              " 26,\n",
              " 40,\n",
              " 52,\n",
              " 63,\n",
              " 12,\n",
              " 23,\n",
              " 36,\n",
              " 48,\n",
              " 61,\n",
              " 13,\n",
              " 26,\n",
              " 38,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 23,\n",
              " 37,\n",
              " 50,\n",
              " 63,\n",
              " 25,\n",
              " 38,\n",
              " 51,\n",
              " 63,\n",
              " 75,\n",
              " 26,\n",
              " 39,\n",
              " 51,\n",
              " 63,\n",
              " 76,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 50,\n",
              " 62,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 48,\n",
              " 59,\n",
              " 13,\n",
              " 25,\n",
              " 36,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 25,\n",
              " 37,\n",
              " 48,\n",
              " 60,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 47,\n",
              " 59,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 49,\n",
              " 63,\n",
              " 12,\n",
              " 24,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 49,\n",
              " 61,\n",
              " 13,\n",
              " 25,\n",
              " 37,\n",
              " 48,\n",
              " 61,\n",
              " 12,\n",
              " 24,\n",
              " 36,\n",
              " 47,\n",
              " 60,\n",
              " 12,\n",
              " 25,\n",
              " 36,\n",
              " 49,\n",
              " 61,\n",
              " 12,\n",
              " 23,\n",
              " 35,\n",
              " 47,\n",
              " 60,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGRJNADN8RGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_story_len = max(all_story_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xljyQNTh8WpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1cc3e4d-a6a1-4c26-d418-e77221b8eb29"
      },
      "source": [
        "max_story_len"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBGTPxcZ79v3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_question_len = [len(data[1]) for data in all_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_LdAAZL8buL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_question_len = max(all_question_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eIrSbVA8g3O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "156da1ff-b04f-42db-ede0-fbabe7d8926f"
      },
      "source": [
        "max_question_len"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibBgu4QOELen",
        "colab_type": "text"
      },
      "source": [
        "##Part 2 \n",
        "\n",
        "* Create a function to vectorize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPn0z6cDENSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B8IFQq5PgSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(filters=[]) # we do not want the text to be cleaned of punctuations\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3iw1s3FP8Pl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "4701c4fe-259b-47fd-f00a-4075df524b11"
      },
      "source": [
        "tokenizer.word_index #maps every word in the vocabulary to an index number"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 9,\n",
              " '?': 27,\n",
              " 'apple': 21,\n",
              " 'back': 13,\n",
              " 'bathroom': 22,\n",
              " 'bedroom': 35,\n",
              " 'daniel': 12,\n",
              " 'discarded': 28,\n",
              " 'down': 7,\n",
              " 'dropped': 4,\n",
              " 'football': 33,\n",
              " 'garden': 23,\n",
              " 'got': 30,\n",
              " 'grabbed': 16,\n",
              " 'hallway': 18,\n",
              " 'in': 8,\n",
              " 'is': 26,\n",
              " 'john': 5,\n",
              " 'journeyed': 2,\n",
              " 'kitchen': 20,\n",
              " 'left': 29,\n",
              " 'mary': 24,\n",
              " 'milk': 17,\n",
              " 'moved': 31,\n",
              " 'no': 36,\n",
              " 'office': 37,\n",
              " 'picked': 10,\n",
              " 'put': 1,\n",
              " 'sandra': 19,\n",
              " 'the': 11,\n",
              " 'there': 25,\n",
              " 'to': 32,\n",
              " 'took': 6,\n",
              " 'travelled': 34,\n",
              " 'up': 14,\n",
              " 'went': 15,\n",
              " 'yes': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDhn8EnEQOXo",
        "colab_type": "text"
      },
      "source": [
        "##Perform Tokenization for story question and answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-QH6GhNQTIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_story_text = []\n",
        "train_question_text = []\n",
        "train_answer_text = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0MINCq7QdHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for story,question,answer in train_data:\n",
        "  train_story_text.append(story)\n",
        "  train_question_text.append(question)\n",
        "  train_answer_text.append(answer)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hewt5kPQ6pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1fc9dcbd-7f3c-4a9c-ef6b-33bf4c4cff19"
      },
      "source": [
        "train_story_text[0]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mary',\n",
              " 'moved',\n",
              " 'to',\n",
              " 'the',\n",
              " 'bathroom',\n",
              " '.',\n",
              " 'Sandra',\n",
              " 'journeyed',\n",
              " 'to',\n",
              " 'the',\n",
              " 'bedroom',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3TLz5tvRach",
        "colab_type": "text"
      },
      "source": [
        "##Generate Sequences for story,question,answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFUm28s5RexM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_story_sequence = tokenizer.texts_to_sequences(train_story_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzzvUdF7Rtpx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4d58732-014c-42ec-9823-68c7c2f29148"
      },
      "source": [
        "len(train_story_sequence)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zalaLRsmS_Zc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ffa2362-d1ff-43e2-e81c-d35317045ded"
      },
      "source": [
        "train_story_sequence[0]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24, 31, 32, 11, 22, 9, 19, 2, 32, 11, 35, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb2pDI5fR4Bg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2ed9002-119a-4e33-c89a-e52762e6c377"
      },
      "source": [
        "len(train_story_text)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc2aeWGRTC6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e44ea1e1-4466-45e4-9e99-4c361384194f"
      },
      "source": [
        "train_story_text[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mary',\n",
              " 'moved',\n",
              " 'to',\n",
              " 'the',\n",
              " 'bathroom',\n",
              " '.',\n",
              " 'Sandra',\n",
              " 'journeyed',\n",
              " 'to',\n",
              " 'the',\n",
              " 'bedroom',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IndkzEELS7gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_stories(data,word_index=tokenizer.word_index,max_story_len=max_story_len,max_question_len = max_question_len ):\n",
        "  #story\n",
        "  X = []\n",
        "  #question\n",
        "  Xq = []\n",
        "  #Answer\n",
        "  Y = []\n",
        "  for story,question,answer in data:\n",
        "    x = [word_index[word.lower()] for word in story]\n",
        "    xq = [word_index[word.lower()] for word in question]\n",
        "    y = np.zeros(len(word_index )+ 1)\n",
        "    y[word_index[answer]] = 1\n",
        "    #\n",
        "    X.append(x)\n",
        "    #\n",
        "    Xq.append(xq)\n",
        "    #\n",
        "    Y.append(y)\n",
        "  return (pad_sequences(X,maxlen=max_story_len),pad_sequences(Xq,maxlen=max_question_len),np.array(Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_tUXt9jYjyR",
        "colab_type": "text"
      },
      "source": [
        "## Vectorize test and train stories,questions and answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hml3P8arVj6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs_train,queries_train,answers_train = vectorize_stories(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz5PgmYiXfie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs_test,queries_test,answers_test = vectorize_stories(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNeJ3G9SX2yr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f61f554f-efaf-464f-c81f-24deacedba58"
      },
      "source": [
        "answers_test"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nsVH83DX7BH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0a033e6-322c-4c5e-80c3-e7804f79e053"
      },
      "source": [
        "tokenizer.word_index['yes']"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iotTx0VlX_OK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35208ef3-06ab-4a22-8c60-4d646d377100"
      },
      "source": [
        "tokenizer.word_index['no']"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy0eFVs0YJTC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b157f64e-85d2-40ec-f724-4bc1ce2ac9ae"
      },
      "source": [
        "sum(answers_test)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.,   0.,   0., 497.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0., 503.,   0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8BycTCOYTUy",
        "colab_type": "text"
      },
      "source": [
        "#### Note there area 497 ==> yes at index location 3 and 503 ==> no at index location 36"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf_1I2TuYq6D",
        "colab_type": "text"
      },
      "source": [
        "## Build Neural Network\n",
        "\n",
        "* input encoder M\n",
        "* input encoder C\n",
        "* Question Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYZOlE3jYS1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential,Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input,Activation,Dense,Permute,Dropout,add,dot,concatenate,LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AbHSY-yZ6Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Place Holder - Input Stories --> shape -->(max_story_len,batch_size)\n",
        "input_sequence = Input((max_story_len,))\n",
        "question = Input((max_question_len,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxKNH33ndNd3",
        "colab_type": "text"
      },
      "source": [
        "## Create input encoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxpgWfkLdMy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Y7Y0d5fSqP",
        "colab_type": "text"
      },
      "source": [
        "##Input Encoder M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqD07Fp5dgjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\n",
        "input_encoder_m.add(Dropout(0.3)) # helps in reducing overfitting\n",
        "#OUTPUT :-->(samples,story_maxlen,embedding_dimensions,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JAyz1DgfVfj",
        "colab_type": "text"
      },
      "source": [
        "## Input Encoder C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu-r8eTpfX6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\n",
        "input_encoder_c.add(Dropout(0.3)) # helps in reducing overfitting\n",
        "# OUTPUT :-->(samples,story_maxlen,max_question_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45E_Th5QgUzz",
        "colab_type": "text"
      },
      "source": [
        "## Question Encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbJeku43gaZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size,output_dim=64,input_length=max_question_len))\n",
        "question_encoder.add(Dropout(0.3)) # helps in reducing overfitting\n",
        "# OUTPUT :-->(samples,question_maxlen,emdedding_dimensions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDDJf9H2h50B",
        "colab_type": "text"
      },
      "source": [
        "## encoded <--encoder(INPUT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmSLBBmJhoy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-pL104miXSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "match = dot([input_encoded_m,question_encoded],axes=(2,2))\n",
        "match = Activation('softmax')(match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suRwwFqRjNiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = add([match,input_encoded_c])\n",
        "response = Permute((2,1))(response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zt4G1CYj2Gf",
        "colab_type": "text"
      },
      "source": [
        "## Concatenation Match Sequence with question vector sequence to get the answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfSDch_rkJzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer = concatenate([response,question_encoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQCgrD-xk-sG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9bfbf7e4-346f-4778-8bf2-3dc30a96fc55"
      },
      "source": [
        "answer"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'concatenate_2/concat:0' shape=(?, 6, 220) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6woCuk4dlRBt",
        "colab_type": "text"
      },
      "source": [
        "## Use a LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhqKDi4wlTUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer = LSTM(38)(answer)\n",
        "answer = Dropout(vocab_size)(answer) #(samples,vocab_size) #YES/NO 0/1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm0RQ1HJlsHA",
        "colab_type": "text"
      },
      "source": [
        "We will see a probability of YES / NO , in order to convert it into a 0/1 we will use softmax activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4srSVdY1l2DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer = Activation('softmax')(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_y8t5_Wl_Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model([input_sequence,question],answer)\n",
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUbbwVv0mhsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "99b0fa55-4acd-4f06-8f08-0532b86ef48d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 156)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 6)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_5 (Sequential)       multiple             2432        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_7 (Sequential)       (None, 6, 64)        2432        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, 156, 6)       0           sequential_5[1][0]               \n",
            "                                                                 sequential_7[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 156, 6)       0           dot_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sequential_6 (Sequential)       multiple             228         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 156, 6)       0           activation_4[0][0]               \n",
            "                                                                 sequential_6[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_2 (Permute)             (None, 6, 156)       0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 6, 220)       0           permute_2[0][0]                  \n",
            "                                                                 sequential_7[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 38)           39368       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 38)           0           lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 38)           0           dropout_8[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 44,460\n",
            "Trainable params: 44,460\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51ixIw1qm_T5",
        "colab_type": "text"
      },
      "source": [
        "##Part 4\n",
        "\n",
        "* Fit / Train the network\n",
        "\n",
        "* Plot Training history\n",
        "\n",
        "* Evaluate on test set\n",
        "\n",
        "* Create own stories and questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM5kHZDMnPrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f14c8d4-3055-458d-8216-46ca1fbaa95f"
      },
      "source": [
        "history = model.fit([inputs_train,queries_train],answers_train,batch_size=32,epochs=1000,validation_data=([inputs_test,queries_test],answers_test))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/1000\n",
            "10000/10000 [==============================] - 4s 439us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 2/1000\n",
            "10000/10000 [==============================] - 4s 430us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 3/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 4/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 5/1000\n",
            "10000/10000 [==============================] - 4s 430us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 6/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 7/1000\n",
            "10000/10000 [==============================] - 4s 435us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 8/1000\n",
            "10000/10000 [==============================] - 4s 434us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 9/1000\n",
            "10000/10000 [==============================] - 4s 433us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 10/1000\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 11/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 12/1000\n",
            "10000/10000 [==============================] - 4s 435us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 13/1000\n",
            "10000/10000 [==============================] - 4s 435us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 14/1000\n",
            "10000/10000 [==============================] - 4s 434us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 15/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 16/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 17/1000\n",
            "10000/10000 [==============================] - 4s 437us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 18/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 19/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 20/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 21/1000\n",
            "10000/10000 [==============================] - 4s 434us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 22/1000\n",
            "10000/10000 [==============================] - 4s 434us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 23/1000\n",
            "10000/10000 [==============================] - 4s 435us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 24/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 25/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 26/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 27/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 28/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 29/1000\n",
            "10000/10000 [==============================] - 4s 439us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 30/1000\n",
            "10000/10000 [==============================] - 4s 437us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 31/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 32/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 33/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 34/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 35/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 36/1000\n",
            "10000/10000 [==============================] - 4s 433us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 37/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 38/1000\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 39/1000\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 40/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 41/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 42/1000\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 43/1000\n",
            "10000/10000 [==============================] - 4s 435us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 44/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 45/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 46/1000\n",
            "10000/10000 [==============================] - 4s 439us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 47/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 48/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 49/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 50/1000\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 51/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 52/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 53/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 54/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 55/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 56/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 57/1000\n",
            "10000/10000 [==============================] - 4s 439us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 58/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 59/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 60/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 61/1000\n",
            "10000/10000 [==============================] - 4s 437us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 62/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 63/1000\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 64/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 65/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 66/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 67/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 68/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 69/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 70/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 71/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 72/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 73/1000\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 74/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 75/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 76/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 77/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 78/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 79/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 80/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 81/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 82/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 83/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 84/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 85/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 86/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 87/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 88/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 89/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 90/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 91/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 92/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 93/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 94/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 95/1000\n",
            "10000/10000 [==============================] - 4s 439us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 96/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 97/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 98/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 99/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 100/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 101/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 102/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 103/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 104/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 105/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 106/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 107/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 108/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 109/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 110/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 111/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 112/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 113/1000\n",
            "10000/10000 [==============================] - 4s 437us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 114/1000\n",
            "10000/10000 [==============================] - 4s 438us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 115/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 116/1000\n",
            "10000/10000 [==============================] - 4s 434us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 117/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 118/1000\n",
            "10000/10000 [==============================] - 4s 437us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 119/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 120/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 121/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 122/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 123/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 124/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 125/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 126/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 127/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 128/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 129/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 130/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 131/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 132/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 133/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 134/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 135/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 136/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 137/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 138/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 139/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 140/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 141/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 142/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 143/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 144/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 145/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 146/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 147/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 148/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 149/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 150/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 151/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 152/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 153/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 154/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 155/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 156/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 157/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 158/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 159/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 160/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 161/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 162/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 163/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 164/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 165/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 166/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 167/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 168/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 169/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 170/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 171/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 172/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 173/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 174/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 175/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 176/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 177/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 178/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 179/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 180/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 181/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 182/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 183/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 184/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 185/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 186/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 187/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 188/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 189/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 190/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 191/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 192/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 193/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 194/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 195/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 196/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 197/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 198/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 199/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 200/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 201/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 202/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 203/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 204/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 205/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 206/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 207/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 208/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 209/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 210/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 211/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 212/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 213/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 214/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 215/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 216/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 217/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 218/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 219/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 220/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 221/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 222/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 223/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 224/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 225/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 226/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 227/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 228/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 229/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 230/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 231/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 232/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 233/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 234/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 235/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 236/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 237/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 238/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 239/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 240/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 241/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 242/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 243/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 244/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 245/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 246/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 247/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 248/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 249/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 250/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 251/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 252/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 253/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 254/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 255/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 256/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 257/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 258/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 259/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 260/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 261/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 262/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 263/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 264/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 265/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 266/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 267/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 268/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 269/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 270/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 271/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 272/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 273/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 274/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 275/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 276/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 277/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 278/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 279/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 280/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 281/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 282/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 283/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 284/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 285/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 286/1000\n",
            "10000/10000 [==============================] - 5s 468us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 287/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 288/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 289/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 290/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 291/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 292/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 293/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 294/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 295/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 296/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 297/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 298/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 299/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 300/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 301/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 302/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 303/1000\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 304/1000\n",
            "10000/10000 [==============================] - 5s 476us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 305/1000\n",
            "10000/10000 [==============================] - 5s 479us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 306/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 307/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 308/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 309/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 310/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 311/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 312/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 313/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 314/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 315/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 316/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 317/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 318/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 319/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 320/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 321/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 322/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 323/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 324/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 325/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 326/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 327/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 328/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 329/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 330/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 331/1000\n",
            "10000/10000 [==============================] - 5s 468us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 332/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 333/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 334/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 335/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 336/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 337/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 338/1000\n",
            "10000/10000 [==============================] - 5s 468us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 339/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 340/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 341/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 342/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 343/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 344/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 345/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 346/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 347/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 348/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 349/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 350/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 351/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 352/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 353/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 354/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 355/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 356/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 357/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 358/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 359/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 360/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 361/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 362/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 363/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 364/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 365/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 366/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 367/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 368/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 369/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 370/1000\n",
            "10000/10000 [==============================] - 5s 474us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 371/1000\n",
            "10000/10000 [==============================] - 5s 492us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 372/1000\n",
            "10000/10000 [==============================] - 5s 484us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 373/1000\n",
            "10000/10000 [==============================] - 5s 468us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 374/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 375/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 376/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 377/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 378/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 379/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 380/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 381/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 382/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 383/1000\n",
            "10000/10000 [==============================] - 5s 472us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 384/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 385/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 386/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 387/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 388/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 389/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 390/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 391/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 392/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 393/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 394/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 395/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 396/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 397/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 398/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 399/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 400/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 401/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 402/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 403/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 404/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 405/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 406/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 407/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 408/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 409/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 410/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 411/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 412/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 413/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 414/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 415/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 416/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 417/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 418/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 419/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 420/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 421/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 422/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 423/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 424/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 425/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 426/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 427/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 428/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 429/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 430/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 431/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 432/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 433/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 434/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 435/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 436/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 437/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 438/1000\n",
            "10000/10000 [==============================] - 5s 480us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 439/1000\n",
            "10000/10000 [==============================] - 5s 477us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 440/1000\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 441/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 442/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 443/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 444/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 445/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 446/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 447/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 448/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 449/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 450/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 451/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 452/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 453/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 454/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 455/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 456/1000\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 457/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 458/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 459/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 460/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 461/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 462/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 463/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 464/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 465/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 466/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 467/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 468/1000\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 469/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 470/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 471/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 472/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 473/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 474/1000\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 475/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 476/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 477/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 478/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 479/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 480/1000\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 481/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 482/1000\n",
            "10000/10000 [==============================] - 5s 467us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 483/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 484/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 485/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 486/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 487/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 488/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 489/1000\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 490/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 491/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 492/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 493/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 494/1000\n",
            "10000/10000 [==============================] - 5s 471us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 495/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 496/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 497/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 498/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 499/1000\n",
            "10000/10000 [==============================] - 5s 463us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 500/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 501/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 502/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 503/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 504/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 505/1000\n",
            "10000/10000 [==============================] - 5s 482us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 506/1000\n",
            "10000/10000 [==============================] - 5s 482us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 507/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 508/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 509/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 510/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 511/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 512/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 513/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 514/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 515/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 516/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 517/1000\n",
            "10000/10000 [==============================] - 5s 487us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 518/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 519/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 520/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 521/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 522/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 523/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 524/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 525/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 526/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 527/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 528/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 529/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 530/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 531/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 532/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 533/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 534/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 535/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 536/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 537/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 538/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 539/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 540/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 541/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 542/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 543/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 544/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 545/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 546/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 547/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 548/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 549/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 550/1000\n",
            "10000/10000 [==============================] - 4s 436us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 551/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 552/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 553/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 554/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 555/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 556/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 557/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 558/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 559/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 560/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 561/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 562/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 563/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 564/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 565/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 566/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 567/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 568/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 569/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 570/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 571/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 572/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 573/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 574/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 575/1000\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 576/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 577/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 578/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 579/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 580/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 581/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 582/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 583/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 584/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 585/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 586/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 587/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 588/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 589/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 590/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 591/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 592/1000\n",
            "10000/10000 [==============================] - 5s 460us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 593/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 594/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 595/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 596/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 597/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 598/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 599/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 600/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 601/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 602/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 603/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 604/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 605/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 606/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 607/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 608/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 609/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 610/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 611/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 612/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 613/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 614/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 615/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 616/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 617/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 618/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 619/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 620/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 621/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 622/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 623/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 624/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 625/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 626/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 627/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 628/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 629/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 630/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 631/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 632/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 633/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 634/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 635/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 636/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 637/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 638/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 639/1000\n",
            "10000/10000 [==============================] - 5s 470us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 640/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 641/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 642/1000\n",
            "10000/10000 [==============================] - 5s 469us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 643/1000\n",
            "10000/10000 [==============================] - 5s 465us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 644/1000\n",
            "10000/10000 [==============================] - 5s 473us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 645/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 646/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 647/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 648/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 649/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 650/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 651/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 652/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 653/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 654/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 655/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 656/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 657/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 658/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 659/1000\n",
            "10000/10000 [==============================] - 5s 458us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 660/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 661/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 662/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 663/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 664/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 665/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 666/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 667/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 668/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 669/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 670/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 671/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 672/1000\n",
            "10000/10000 [==============================] - 5s 464us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 673/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 674/1000\n",
            "10000/10000 [==============================] - 5s 462us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 675/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 676/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 677/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 678/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 679/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 680/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 681/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 682/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 683/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 684/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 685/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 686/1000\n",
            "10000/10000 [==============================] - 5s 456us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 687/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 688/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 689/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 690/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 691/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 692/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 693/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 694/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 695/1000\n",
            "10000/10000 [==============================] - 4s 444us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 696/1000\n",
            "10000/10000 [==============================] - 4s 441us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 697/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 698/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 699/1000\n",
            "10000/10000 [==============================] - 5s 454us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 700/1000\n",
            "10000/10000 [==============================] - 5s 455us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 701/1000\n",
            "10000/10000 [==============================] - 4s 445us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 702/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 703/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 704/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 705/1000\n",
            "10000/10000 [==============================] - 4s 440us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 706/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 707/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 708/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 709/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 710/1000\n",
            "10000/10000 [==============================] - 4s 439us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 711/1000\n",
            "10000/10000 [==============================] - 5s 457us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 712/1000\n",
            "10000/10000 [==============================] - 5s 466us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 713/1000\n",
            "10000/10000 [==============================] - 5s 461us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 714/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 715/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 716/1000\n",
            "10000/10000 [==============================] - 5s 453us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 717/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 718/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 719/1000\n",
            "10000/10000 [==============================] - 4s 448us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 720/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 721/1000\n",
            "10000/10000 [==============================] - 4s 449us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 722/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 723/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 724/1000\n",
            "10000/10000 [==============================] - 5s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 725/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 726/1000\n",
            "10000/10000 [==============================] - 5s 459us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 727/1000\n",
            "10000/10000 [==============================] - 5s 451us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 728/1000\n",
            "10000/10000 [==============================] - 5s 452us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 729/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 730/1000\n",
            "10000/10000 [==============================] - 4s 447us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 731/1000\n",
            "10000/10000 [==============================] - 4s 442us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 732/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 733/1000\n",
            "10000/10000 [==============================] - 4s 450us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 734/1000\n",
            "10000/10000 [==============================] - 4s 443us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 735/1000\n",
            "10000/10000 [==============================] - 4s 446us/step - loss: 1.9275 - acc: 0.5012 - val_loss: 1.9275 - val_acc: 0.4970\n",
            "Epoch 736/1000\n",
            " 8928/10000 [=========================>....] - ETA: 0s - loss: 1.9275 - acc: 0.5002Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzufGPmmrRHZ",
        "colab_type": "text"
      },
      "source": [
        "##Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXTCjsmFrTL-",
        "colab_type": "text"
      },
      "source": [
        "#### Plot training History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfOMfTeMrWph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "adc24d75-bec2-4487-9853-d331f59efb2b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "plt.plot(history.history['acc'],label='Training Accuracy')\n",
        "plt.plot(history.history['val_acc'],label='Validation Accuracy')\n",
        "plt.title('model accuracy')\n",
        "plt.xlabel('number of epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}